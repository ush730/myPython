{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW\n",
    "* Bag Of Words\n",
    "* 단어의 인덱스를 기준으로 토큰의 출현빈도를 나타낸 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DictVectorizer\n",
    "* 문서에서 단어의 출현빈도를 딕셔너리로 입력받아 BOW 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 3., 2.],\n",
       "       [2., 0., 5., 4., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "d = [\n",
    "    {'java':1, 'python':2, 'mysql':3},\n",
    "    {'mysql':4, 'linux':5, 'html':2}\n",
    "]\n",
    "x = v.fit_transform(d)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['html', 'java', 'linux', 'mysql', 'python']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.,   0.,   0.,   0., 100.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'html':10, 'python':100, 'javascript':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "* 문서에서 단어토큰을 리스트로 생성하고, 각 단어들의 수를 BOW 벡터로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 10,\n",
       " 'is': 5,\n",
       " 'the': 8,\n",
       " 'first': 3,\n",
       " 'document': 1,\n",
       " 'second': 7,\n",
       " 'and': 0,\n",
       " 'third': 9,\n",
       " 'documentthis': 2,\n",
       " 'fourth': 4,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document',\n",
    "    'This is the second document',\n",
    "    'and the third document'\n",
    "    'this is the fourth document',\n",
    "    'the last document'\n",
    "]\n",
    "v = CountVectorizer()\n",
    "v.fit(corpus)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 인덱스에 해당하는 단어 횟수\n",
    "v.transform(['this is the first first document']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform(['i am a boy']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 0,\n",
       " 'second': 5,\n",
       " 'third': 7,\n",
       " 'documentthis': 1,\n",
       " 'fourth': 3,\n",
       " 'last': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어\n",
    "v2 = CountVectorizer(stop_words=['and', 'is'])\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 14,\n",
       " 'h': 6,\n",
       " 'i': 7,\n",
       " 's': 13,\n",
       " ' ': 0,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'r': 12,\n",
       " 'd': 3,\n",
       " 'o': 11,\n",
       " 'c': 2,\n",
       " 'u': 15,\n",
       " 'm': 9,\n",
       " 'n': 10,\n",
       " 'a': 1,\n",
       " 'l': 8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자\n",
    "v2 = CountVectorizer(analyzer='char')\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 1, 'third': 2, 'tthis': 4, 'th': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식\n",
    "v2 = CountVectorizer(token_pattern='t\\w+')\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "C:\\Users\\sundooedu\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 10,\n",
       " 'is': 5,\n",
       " 'the': 8,\n",
       " 'first': 3,\n",
       " 'document': 1,\n",
       " 'second': 7,\n",
       " 'and': 0,\n",
       " 'third': 9,\n",
       " 'documentthis': 2,\n",
       " 'fourth': 4,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "v2 = CountVectorizer(tokenizer=nltk.word_tokenize)\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 13,\n",
       " 'is the': 4,\n",
       " 'the first': 7,\n",
       " 'first document': 2,\n",
       " 'the second': 10,\n",
       " 'second document': 6,\n",
       " 'and the': 0,\n",
       " 'the third': 11,\n",
       " 'third documentthis': 12,\n",
       " 'documentthis is': 1,\n",
       " 'the fourth': 8,\n",
       " 'fourth document': 3,\n",
       " 'the last': 9,\n",
       " 'last document': 5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram\n",
    "# 토큰의 크기\n",
    "# monogram - 토큰하나, bigram - 토큰두개\n",
    "v2 = CountVectorizer(ngram_range=(2,2))\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 10,\n",
       " 'is': 5,\n",
       " 'the': 8,\n",
       " 'first': 3,\n",
       " 'document': 1,\n",
       " 'second': 7,\n",
       " 'and': 0,\n",
       " 'third': 9,\n",
       " 'documentthis': 2,\n",
       " 'fourth': 4,\n",
       " 'last': 6}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도\n",
    "v2 = CountVectorizer(min_df=1, max_df=4)\n",
    "v2.fit(corpus)\n",
    "v2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, 1, 1, 3, 1, 1, 5, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "* Tf-idf (Term Frequency-Inverse Document Frequency)\n",
    "* 단어의 가중치를 조절한 BOW 벡터 생성\n",
    "* 단어빈도수로만 보는것이 아니라, 전체 문서에 포함된 단어의 가중치를 줄이는 방식\n",
    "* tf(단어빈도수)\n",
    "* idf(특정단어가 들어있는 문서의 수에 반비례) = 문서수 / 단어가포함된문서수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.32528549, 0.        , 0.62334157, 0.        ,\n",
       "        0.39787085, 0.        , 0.        , 0.32528549, 0.        ,\n",
       "        0.49144966],\n",
       "       [0.        , 0.32528549, 0.        , 0.        , 0.        ,\n",
       "        0.39787085, 0.        , 0.62334157, 0.32528549, 0.        ,\n",
       "        0.49144966],\n",
       "       [0.41634142, 0.21726422, 0.41634142, 0.        , 0.41634142,\n",
       "        0.26574533, 0.        , 0.        , 0.43452845, 0.41634142,\n",
       "        0.        ],\n",
       "       [0.        , 0.41988018, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.8046125 , 0.        , 0.41988018, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ti = TfidfVectorizer()\n",
    "ti.fit(corpus)\n",
    "ti.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HashingVectorizer\n",
    "* 해쉬함수를 이용해서 BOW 벡터 생성\n",
    "* 빠른 속도, 적은 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups().data\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CountVectorizer().fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.57 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x2000000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1786633 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "HashingVectorizer(n_features=2000000).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
