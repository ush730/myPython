{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 감정분석\n",
    "* 텍스트에 숨겨져 있는 저장의 의도(감정, 정보)를 찾아내는 방법\n",
    "* Opinion mining, Sentiment mining\n",
    "* 지식기반(knowlege-base) : 사람(전문가)\n",
    "* 머신러닝기반(machine learning-base) : 대표적으로 긍정/부정\n",
    "    * text classification(텍스트 분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 분류\n",
    "* 벡터형태로 변환된 텍스트를 분류하는 방법\n",
    "* DNN이 나오기 전까지는 SVM 많이 사용\n",
    "* 최근은 딥러닝(RNN, LSTM) 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_text.pk', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('test_text.pk', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 48760)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['진짜', '짜증', '네요', '목소리'], '0'),\n",
       " (['포스터', '보고', '초딩', '영화', '오버', '연기', '가볍', '구나'], '1'),\n",
       " (['너무', '밓었', '추천', '한다'], '0'),\n",
       " (['교도소', '이야기', '구먼', '솔직히', '재미', '평점', '조정'], '0'),\n",
       " (['사이몬페그',\n",
       "   '익살',\n",
       "   '스런',\n",
       "   '연기',\n",
       "   '돋보였',\n",
       "   '영화',\n",
       "   '스파이더맨',\n",
       "   '보이',\n",
       "   '커스틴',\n",
       "   '던스트',\n",
       "   '너무나',\n",
       "   '이뻐',\n",
       "   '보였'],\n",
       "  '1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['평점', '나쁘', '짜리', '더더욱', '잖아'], '0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMC 분류\n",
    "* 텍스트를 벡터화 머신러닝\n",
    "* pipeline 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리(학습/테스트셋 별 x,y)\n",
    "train_x = [' '.join(t) for t,c in train] # 학습데이터 문제\n",
    "train_y = [c for t,c in train] # 학습데이터 정답\n",
    "test_x = [' '.join(t) for t,c in test] # 테스트데이터 문제\n",
    "test_y = [c for t,c in test] # 테스트데이터 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['평점 나쁘 짜리 더더욱 잖아',\n",
       " '지루 은데 완전 막장',\n",
       " '어도 텐데 나와서 심기 불편',\n",
       " '음악 주가 최고 음악 영화',\n",
       " '진정 쓰레기']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인\n",
    "model1 = Pipeline([\n",
    "    ('vector', CountVectorizer()),\n",
    "    ('model', SVC(kernel='linear'))\n",
    "])\n",
    "model2 = Pipeline([\n",
    "    ('vector', CountVectorizer()),\n",
    "    ('model', SVC(kernel='sigmoid'))\n",
    "])\n",
    "model3 = Pipeline([\n",
    "    ('vector', CountVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 정확도:0.87698\n",
      "테스트데이터 정확도:0.8106439704675964\n"
     ]
    }
   ],
   "source": [
    "# model 변수에 파이프라인객체 대입\n",
    "model = model1\n",
    "\n",
    "model.fit(train_x, train_y) # 학습 (문제,답)\n",
    "pred_train = model.predict(train_x) # 예측\n",
    "train_acc = (pred_train==train_y).mean()\n",
    "print('학습데이터 정확도:{}'.format(train_acc))\n",
    "\n",
    "pred_test = model.predict(test_x) # 학습하지 않은 테스트데이터로 예측\n",
    "test_acc = (pred_test==test_y).mean()\n",
    "print('테스트데이터 정확도:{}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트로 직접 예측\n",
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "model1 = Pipeline([\n",
    "    ('vector', CountVectorizer(tokenizer=mecab.pos)),\n",
    "    ('model', SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file):\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        doc = [line.replace('\\n','').split('\\t') for line in f.readlines()]\n",
    "    return doc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = readFile('ratings_train.txt')\n",
    "test_text = readFile('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [text[1] for text in train_text]\n",
    "train_y = [text[2] for text in train_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vector',\n",
       "                 CountVectorizer(tokenizer=<bound method Mecab.pos of <eunjeon._mecab.Mecab object at 0x000001ED247F7130>>)),\n",
       "                ('model', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_x[:1000], train_y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = np.array([\n",
    "    '지루하지는 않은데 완전 막장임... 돈주고 보기에는....',\n",
    "    '음악이 주가 된, 최고의 음악영화',\n",
    "    '진정한 쓰레기'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '0'], dtype='<U1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype='<U1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(['넌 정말 착하고 재밌는 사람이야'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype='<U1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(['넌 정말 쓰레기같은 사람이야'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 모델로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "max_words = 35000\n",
    "max_len = 50\n",
    "batch_size=128\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 토크나이징, 패딩(제로패딩)\n",
    "import numpy as np\n",
    "\n",
    "# 문서만 추출\n",
    "train_x = [doc for doc, _ in train]\n",
    "\n",
    "# 토크나이징\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# 변환(LSTM모델의 입력값을 넣기 위해)\n",
    "x_train = tokenizer.texts_to_sequences(train_x)\n",
    "y_train = np.array([int(label) for _, label in train])\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences([doc for doc, _ in test])\n",
    "y_test = np.array([int(label) for _, label in test])\n",
    "\n",
    "# 길이를 동일하게 맞추기\n",
    "x_train = pad_sequences(x_train, maxlen=max_len, padding='pre')\n",
    "x_test = pad_sequences(x_test, maxlen=max_len, padding='pre')\n",
    "\n",
    "# 학습가능한 형태로 변환\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 50), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "# 클래스로 구현하는 방법\n",
    "class MyLSTM(tf.keras.Model):\n",
    "    # 생성자\n",
    "    def __init__(self):\n",
    "        # 부모클래스의 생성자 호출\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(max_words, 100)\n",
    "        self.lstm = LSTM(128, dropout=0.2, recurrent_dropout=0.2)\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델객체 선언\n",
    "model = MyLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델설정\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3, verbose=1)\n",
    "# ModelCheckpoint\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('nsmc_bestmodel.cp', monitor='val_loss', mode='min', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1172/1172 [==============================] - 176s 147ms/step - loss: 0.4777 - acc: 0.7561 - val_loss: 0.3926 - val_acc: 0.8193\n",
      "Epoch 2/30\n",
      "1172/1172 [==============================] - 160s 137ms/step - loss: 0.3568 - acc: 0.8369 - val_loss: 0.3991 - val_acc: 0.8187\n",
      "Epoch 3/30\n",
      "1172/1172 [==============================] - 165s 141ms/step - loss: 0.3089 - acc: 0.8622 - val_loss: 0.4312 - val_acc: 0.8156\n",
      "Epoch 4/30\n",
      "1172/1172 [==============================] - 165s 141ms/step - loss: 0.2635 - acc: 0.8815 - val_loss: 0.4789 - val_acc: 0.8097\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=epochs, batch_size=batch_size, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(text):\n",
    "    import text_module\n",
    "    text = text_module.preprocessing(text)\n",
    "    text = tokenizer.texts_to_sequences([text])\n",
    "    text = pad_sequences(text, maxlen=max_len, padding='pre')\n",
    "    if model.predict(text) > 0.5:\n",
    "        print('긍정')\n",
    "    else :\n",
    "        print('부정')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정\n"
     ]
    }
   ],
   "source": [
    "predict_func('정말 최고의 영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정\n"
     ]
    }
   ],
   "source": [
    "predict_func('정말 최악의 영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
